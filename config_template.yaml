training:
  output_dir: 'output'
  seed: 42
  compile: False                 # use PyTorch >= 2.0 to compile the model to be faster

  init_from: 'scratch'           # 'scratch' or 'resume'
  init_iter: 0                   # if resuming, which iteration to start from
  max_iters: 60000              # total number of training iterations

  gradient_accumulation_steps: 5    # used to simulate larger batch sizes
  batch_size: 12                     # if gradient_accumulation_steps > 1, this is the micro-batch size

  eval_interval: 1000
  eval_iters: 200
  log_interval: 10
  always_save_checkpoint: True   # if True, always save a checkpoint after each eval

  eval_only: False               # if True, script exits right after the first eval

wandb:
  wandb_log: False       # disabled by default
  wandb_project: 'owt'
  wandb_run_name: 'gpt2' # 'run' + str(time.time())

data:
  data_dir: '../openwebtext'
  context_length: 512
  vocab_size: 50257

model:
  n_layer: 8
  n_head: 8
  embedding_dim: 512
  dropout: 0.0   # for pretraining 0 is good, for finetuning try 0.1+
  bias: False    # bias inside LayerNorm and Linear layers

optimizer:
  optimizer: AdamW     # "AdamW" or "Adam"
  learning_rate: !!float 1e-3  # max learning rate
  weight_decay: !!float 1e-1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0       # clip gradients at this value, or disable if:= 0.0
  fused: True          # use torch fused version optimizer

scheduler:
  decay_lr: True           # whether to decay the learning rate
  warmup_iters: 1000       # how many steps to warm up for
  lr_decay_iters: 6000   # should be ~= max_iters per Chinchilla
  min_lr: !!float 6e-5     # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

distributed:
  parallel: False    # use nn.DataParallel for multi-GPU training
  distributed: False # use torch.nn.parallel.DistributedDataParallel for multi-GPU training
  backend: 'nccl'    # 'nccl', 'gloo', 'mpi', 'tcp' etc.

system:
  device: 'cuda'      # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
  dtype: 'bfloat16'   # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
